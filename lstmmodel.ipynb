{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5bFnOaYglGiKBbn9LykTv"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install keras --quiet\n",
        "!pip install tensorflow --quiet"
      ],
      "metadata": {
        "id": "8ZGfwgFUjrFU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuxI8Ws2_ydU",
        "outputId": "09593a41-66c5-450a-d0f1-91dc57d060a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbRd8630fcm5",
        "outputId": "4e69d770-0718-40ff-96cf-8cbff48b5968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from gensim.models import FastText\n",
        "from gensim.models import KeyedVectors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('arxiv_data.csv')\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvgM15ICjIAX",
        "outputId": "fc4a3d13-3a42-4597-e70d-59e35674a5f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 51774 entries, 0 to 51773\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   titles     51774 non-null  object\n",
            " 1   summaries  51774 non-null  object\n",
            " 2   terms      51774 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 1.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['terms'],inplace =True, axis=1)"
      ],
      "metadata": {
        "id": "rTu8X7fijJBm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(inplace=True)\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0KKnYmAjSnD",
        "outputId": "f954c166-6154-470a-e092-8c2248f4d0cf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['summaries']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mru1CvQulMtG",
        "outputId": "84c38238-a6b9-48be-9763-b831419c7ca6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        Stereo matching is one of the widely used tech...\n",
              "1        The recent advancements in artificial intellig...\n",
              "2        In this paper, we proposed a novel mutual cons...\n",
              "3        Consistency training has proven to be an advan...\n",
              "4        To ensure safety in automated driving, the cor...\n",
              "                               ...                        \n",
              "51767    Diffusion Tensor Imaging (DTI) is a non-invasi...\n",
              "51768    Single molecule fluorescence microscopy is a p...\n",
              "51770    We discuss a method for tracking individual mo...\n",
              "51771    We attempt to set a mathematical foundation of...\n",
              "51772    Diffusion Tensor Imaging (DTI) allows estimati...\n",
              "Name: summaries, Length: 38985, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df= df.iloc[:1000, :]\n"
      ],
      "metadata": {
        "id": "9C6V-Y4KIbxe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]|[\\d]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Join the tokens back into a string\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    return text\n",
        "df['summaries'] = df['summaries'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "ldNBKraHlopS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['summaries'])\n",
        "sequences = tokenizer.texts_to_sequences(df['summaries'])\n",
        "max_len = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Create input and target sequences\n",
        "input_sequences = padded_sequences[:,:-1]\n",
        "target_sequences = padded_sequences[:,1:]"
      ],
      "metadata": {
        "id": "48ErwNJpjogK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# define the input data\n",
        "data = df['summaries']\n",
        "\n",
        "# tokenize the data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data)\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "\n",
        "# prepare the input and output data\n",
        "input_data = []\n",
        "output_data = []\n",
        "for sequence in sequences:\n",
        "    for i in range(1, len(sequence)):\n",
        "        input_data.append(sequence[:i])\n",
        "        output_data.append(sequence[i])\n",
        "input_data = pad_sequences(input_data)\n",
        "output_data = to_categorical(output_data)\n",
        "\n",
        "fasttext = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/wiki-news-300d-1M.vec')\n",
        "\n",
        "# create an embedding matrix for the words in our vocabulary\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in fasttext:\n",
        "        embedding_matrix[i] = fasttext[word]\n",
        "\n",
        "\n",
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(tokenizer.word_index) + 1, embedding_dim, weights=[embedding_matrix], trainable=False))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# fit the model\n",
        "#model.fit(input_data, output_data, epochs=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "RuCYGNR3o2C_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = tf.keras.models.load_model('/content/lstm_model.h5')\n",
        "model.fit(input_data, output_data, initial_epoch=2, epochs=5)\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "model_path = 'lstm_model_2.h5'\n",
        "model.save(model_path)\n",
        "files.download(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "g7yVDVz7jjLA",
        "outputId": "1fa89019-38d7-4a4b-f200-72d93f63de19"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5\n",
            "5484/5484 [==============================] - 3243s 591ms/step - loss: 4.9002\n",
            "Epoch 4/5\n",
            "5484/5484 [==============================] - 3210s 585ms/step - loss: 4.6075\n",
            "Epoch 5/5\n",
            "5484/5484 [==============================] - 3215s 586ms/step - loss: 4.3624\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_65b8b4e0-dba1-4333-9602-e8044f933416\", \"lstm_model_2.h5\", 30132888)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "\n",
        "#model_path = 'lstm_model.h5'\n",
        "#model.save(model_path)\n",
        "#files.download(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "mNV_N9TipdZL",
        "outputId": "6ffe7663-ea87-4654-9aa5-bd6c6607b033"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7764ef24-d82c-47f3-8f01-bce56b1fc068\", \"lstm_model.h5\", 30132888)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tiJUfgsz446S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_words = df['summaries'].str.split().str.len().sum()\n",
        "average_words = df['summaries'].str.split().str.len().mean()\n",
        "print(total_words)\n",
        "print(average_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRF5nI34iyo5",
        "outputId": "ed8b65d4-8ee7-42d6-9a2b-e1e93a3a2302"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "176448\n",
            "176.448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bgu1AQNdS5i-",
        "outputId": "18adc7cd-ccac-43b6-c7c6-3ced57606770"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1000 entries, 0 to 999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   titles     1000 non-null   object\n",
            " 1   summaries  1000 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 23.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['summaries'].sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4TB1bhFTbqN",
        "outputId": "dad5ca07-346f-4830-9927-c60734e2b99e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "391    to assist researchers to identify environmenta...\n",
              "306    magnetic resonance mr protocols rely on severa...\n",
              "273    in image segmentation there is often more than...\n",
              "961    we propose a novel active learning framework c...\n",
              "17     deep neural networks have been a prevailing te...\n",
              "Name: summaries, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# define a function to generate text using the trained model\n",
        "def generate_text(model, tokenizer, input_text, num_words):\n",
        "    for _ in range(num_words):\n",
        "        # tokenize the input text\n",
        "        input_sequence = tokenizer.texts_to_sequences([input_text])[0]\n",
        "        # pad the input sequence\n",
        "        input_sequence = pad_sequences([input_sequence])\n",
        "        # make a prediction\n",
        "        prediction = model.predict(input_sequence)\n",
        "        # get the index of the predicted word\n",
        "        predicted_index = np.argmax(prediction)\n",
        "        # get the predicted word\n",
        "        predicted_word = tokenizer.index_word[predicted_index]\n",
        "        # update the input text\n",
        "        input_text += ' ' + predicted_word\n",
        "    return input_text\n"
      ],
      "metadata": {
        "id": "qxCZUftoTFM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = 'we propose a novel active learning framework'\n",
        "generated_text = generate_text(model, tokenizer, input_text, 10)\n",
        "print(generated_text)\n",
        "\n",
        "# visualize the predictions made by the model\n",
        "input_sequence = tokenizer.texts_to_sequences([input_text])[0]\n",
        "input_sequence = pad_sequences([input_sequence])\n",
        "prediction = model.predict(input_sequence)[0]\n",
        "predicted_indices = np.argsort(prediction)[::-1][:5]\n",
        "predicted_words = [tokenizer.index_word[i] for i in predicted_indices]\n",
        "predicted_probabilities = prediction[predicted_indices]\n",
        "for word, probability in zip(predicted_words, predicted_probabilities):\n",
        "    print(f'{word}: {probability:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbddXs6wTabC",
        "outputId": "c152a07b-3d90-494f-bb44-5103ad855925"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "we propose a novel active learning framework to effectively generate the segmentation masks to be segmented into\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "to: 0.34\n",
            "that: 0.16\n",
            "for: 0.15\n",
            "based: 0.06\n",
            "which: 0.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Titles"
      ],
      "metadata": {
        "id": "F4xKDplUV1Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['titles']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-Erzt4RfhnY",
        "outputId": "80103374-9858-495c-ee8d-5477f0347396"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      survey on semantic stereo matching semantic de...\n",
              "1      futureai guiding principles and consensus reco...\n",
              "2      enforcing mutual consistency of hard regions f...\n",
              "3      parameter decoupling strategy for semisupervis...\n",
              "4      backgroundforeground segmentation for interior...\n",
              "                             ...                        \n",
              "995    deepigeos a deep interactive geodesic framewor...\n",
              "996    d densely convolutional networks for volumetri...\n",
              "997    uinet interactive artificial neural networks f...\n",
              "998           oneshot learning for semantic segmentation\n",
              "999    exploring and exploiting diversity for image s...\n",
              "Name: titles, Length: 1000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['titles'] = df['titles'].apply(preprocess_text)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['titles'])\n",
        "sequences = tokenizer.texts_to_sequences(df['titles'])\n",
        "max_len = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Create input and target sequences\n",
        "input_sequences = padded_sequences[:,:-1]\n",
        "target_sequences = padded_sequences[:,1:]"
      ],
      "metadata": {
        "id": "IMrTOeRRU7zD"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfrm43yHWPRx",
        "outputId": "06cf80b2-ed19-4c22-a823-93b892425779"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  99,   21,   13, ...,    0,    0,    0],\n",
              "       [ 828,  829,  830, ...,    0,    0,    0],\n",
              "       [ 509,  219,  143, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [2269,   38,  342, ...,    0,    0,    0],\n",
              "       [ 319,    5,    2, ...,    0,    0,    0],\n",
              "       [ 826,    4,  477, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def add_end_token(text):\n",
        "    # Add end token after every sentence\n",
        "    text = re.sub(r'([^.]*\\.)', r'\\1 <end>', text)\n",
        "    # Remove trailing spaces and <end> tokens\n",
        "    text = text.strip().replace(' <end>', '<end>')\n",
        "    # Add <end> token if missing\n",
        "    if not text.endswith('<end>'):\n",
        "        text += ' <end>'\n",
        "    return text\n",
        "\n",
        "# Example usage:\n",
        "df['titles'] = df['titles'].apply(add_end_token)\n"
      ],
      "metadata": {
        "id": "1kwdOghsBLr_"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dropout\n",
        "\n",
        "# define the input and output data\n",
        "data = df['titles']\n",
        "\n",
        "# tokenize the data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data)\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "\n",
        "# prepare the input and output data\n",
        "max_sequence_length = max([len(seq) for seq in sequences])\n",
        "input_data = []\n",
        "output_data = []\n",
        "for sequence in sequences:\n",
        "    for i in range(1, len(sequence)):\n",
        "        input_seq = sequence[:i]\n",
        "        input_seq = pad_sequences([input_seq], maxlen=max_sequence_length)[0]\n",
        "        output_seq = to_categorical(sequence[i], num_classes=len(tokenizer.word_index) + 1)\n",
        "        input_data.append(input_seq)\n",
        "        output_data.append(output_seq)\n",
        "input_data = np.array(input_data)\n",
        "output_data = np.array(output_data)\n",
        "\n",
        "fasttext = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/wiki-news-300d-1M.vec')\n",
        "\n",
        "# create an embedding matrix for the words in our vocabulary\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in fasttext:\n",
        "        embedding_matrix[i] = fasttext[word]\n",
        "\n",
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(tokenizer.word_index) + 1, embedding_dim, weights=[embedding_matrix], trainable=True))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#model.fit(input_data, output_data, epochs=40)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzpVxmUyWRjr",
        "outputId": "e0ec77fb-36a7-4700-c757-d8a0faaf4f15"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_8 (Embedding)     (None, None, 300)         681000    \n",
            "                                                                 \n",
            " lstm_9 (LSTM)               (None, None, 256)         570368    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, None, 256)         0         \n",
            "                                                                 \n",
            " lstm_10 (LSTM)              (None, 128)               197120    \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 2270)              292830    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,741,318\n",
            "Trainable params: 1,741,318\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(input_data, output_data, initial_epoch=100, epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyYhEE8_tOJH",
        "outputId": "c3c793d9-29b7-464e-e1c1-64881d93b07b"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 101/200\n",
            "301/301 [==============================] - 81s 252ms/step - loss: 5.9221 - accuracy: 0.1032\n",
            "Epoch 102/200\n",
            "301/301 [==============================] - 77s 255ms/step - loss: 5.2622 - accuracy: 0.1617\n",
            "Epoch 103/200\n",
            "301/301 [==============================] - 72s 239ms/step - loss: 4.8541 - accuracy: 0.2339\n",
            "Epoch 104/200\n",
            "301/301 [==============================] - 74s 247ms/step - loss: 4.5535 - accuracy: 0.2645\n",
            "Epoch 105/200\n",
            "301/301 [==============================] - 75s 250ms/step - loss: 4.3207 - accuracy: 0.2866\n",
            "Epoch 106/200\n",
            "301/301 [==============================] - 84s 278ms/step - loss: 4.1338 - accuracy: 0.2995\n",
            "Epoch 107/200\n",
            "301/301 [==============================] - 83s 275ms/step - loss: 3.9679 - accuracy: 0.3095\n",
            "Epoch 108/200\n",
            "301/301 [==============================] - 82s 273ms/step - loss: 3.8142 - accuracy: 0.3171\n",
            "Epoch 109/200\n",
            "301/301 [==============================] - 79s 263ms/step - loss: 3.6746 - accuracy: 0.3252\n",
            "Epoch 110/200\n",
            "301/301 [==============================] - 74s 247ms/step - loss: 3.5427 - accuracy: 0.3361\n",
            "Epoch 111/200\n",
            "301/301 [==============================] - 74s 247ms/step - loss: 3.4111 - accuracy: 0.3494\n",
            "Epoch 112/200\n",
            "301/301 [==============================] - 71s 238ms/step - loss: 3.2879 - accuracy: 0.3554\n",
            "Epoch 113/200\n",
            "301/301 [==============================] - 75s 249ms/step - loss: 3.1629 - accuracy: 0.3673\n",
            "Epoch 114/200\n",
            "301/301 [==============================] - 73s 244ms/step - loss: 3.0459 - accuracy: 0.3822\n",
            "Epoch 115/200\n",
            "301/301 [==============================] - 75s 248ms/step - loss: 2.9302 - accuracy: 0.3945\n",
            "Epoch 116/200\n",
            "301/301 [==============================] - 73s 243ms/step - loss: 2.8122 - accuracy: 0.4057\n",
            "Epoch 117/200\n",
            "301/301 [==============================] - 76s 251ms/step - loss: 2.7002 - accuracy: 0.4218\n",
            "Epoch 118/200\n",
            "301/301 [==============================] - 72s 238ms/step - loss: 2.5894 - accuracy: 0.4392\n",
            "Epoch 119/200\n",
            "301/301 [==============================] - 75s 251ms/step - loss: 2.4788 - accuracy: 0.4613\n",
            "Epoch 120/200\n",
            "301/301 [==============================] - 76s 251ms/step - loss: 2.3799 - accuracy: 0.4750\n",
            "Epoch 121/200\n",
            "301/301 [==============================] - 77s 255ms/step - loss: 2.2732 - accuracy: 0.4977\n",
            "Epoch 122/200\n",
            "301/301 [==============================] - 77s 254ms/step - loss: 2.1756 - accuracy: 0.5158\n",
            "Epoch 123/200\n",
            "301/301 [==============================] - 76s 253ms/step - loss: 2.0685 - accuracy: 0.5407\n",
            "Epoch 124/200\n",
            "301/301 [==============================] - 76s 252ms/step - loss: 1.9846 - accuracy: 0.5603\n",
            "Epoch 125/200\n",
            "301/301 [==============================] - 78s 258ms/step - loss: 1.8949 - accuracy: 0.5790\n",
            "Epoch 126/200\n",
            "301/301 [==============================] - 77s 255ms/step - loss: 1.8037 - accuracy: 0.5994\n",
            "Epoch 127/200\n",
            "301/301 [==============================] - 77s 254ms/step - loss: 1.7194 - accuracy: 0.6156\n",
            "Epoch 128/200\n",
            "301/301 [==============================] - 76s 253ms/step - loss: 1.6404 - accuracy: 0.6318\n",
            "Epoch 129/200\n",
            "301/301 [==============================] - 77s 256ms/step - loss: 1.5657 - accuracy: 0.6500\n",
            "Epoch 130/200\n",
            "301/301 [==============================] - 74s 245ms/step - loss: 1.4903 - accuracy: 0.6739\n",
            "Epoch 131/200\n",
            "301/301 [==============================] - 78s 258ms/step - loss: 1.4233 - accuracy: 0.6844\n",
            "Epoch 132/200\n",
            "301/301 [==============================] - 75s 250ms/step - loss: 1.3532 - accuracy: 0.7009\n",
            "Epoch 133/200\n",
            "301/301 [==============================] - 74s 246ms/step - loss: 1.2918 - accuracy: 0.7122\n",
            "Epoch 134/200\n",
            "301/301 [==============================] - 78s 260ms/step - loss: 1.2278 - accuracy: 0.7326\n",
            "Epoch 135/200\n",
            "301/301 [==============================] - 79s 263ms/step - loss: 1.1747 - accuracy: 0.7413\n",
            "Epoch 136/200\n",
            "301/301 [==============================] - 82s 272ms/step - loss: 1.1136 - accuracy: 0.7551\n",
            "Epoch 137/200\n",
            "301/301 [==============================] - 80s 267ms/step - loss: 1.0727 - accuracy: 0.7634\n",
            "Epoch 138/200\n",
            "301/301 [==============================] - 78s 259ms/step - loss: 1.0231 - accuracy: 0.7746\n",
            "Epoch 139/200\n",
            "301/301 [==============================] - 78s 258ms/step - loss: 0.9793 - accuracy: 0.7858\n",
            "Epoch 140/200\n",
            "301/301 [==============================] - 75s 249ms/step - loss: 0.9318 - accuracy: 0.7953\n",
            "Epoch 141/200\n",
            "301/301 [==============================] - 75s 250ms/step - loss: 0.8852 - accuracy: 0.8080\n",
            "Epoch 142/200\n",
            "301/301 [==============================] - 78s 258ms/step - loss: 0.8414 - accuracy: 0.8200\n",
            "Epoch 143/200\n",
            "301/301 [==============================] - 77s 257ms/step - loss: 0.8014 - accuracy: 0.8309\n",
            "Epoch 144/200\n",
            "301/301 [==============================] - 73s 244ms/step - loss: 0.7629 - accuracy: 0.8375\n",
            "Epoch 145/200\n",
            "301/301 [==============================] - 73s 242ms/step - loss: 0.7308 - accuracy: 0.8452\n",
            "Epoch 146/200\n",
            "301/301 [==============================] - 73s 242ms/step - loss: 0.6959 - accuracy: 0.8523\n",
            "Epoch 147/200\n",
            "301/301 [==============================] - 74s 247ms/step - loss: 0.6696 - accuracy: 0.8569\n",
            "Epoch 148/200\n",
            "301/301 [==============================] - 74s 247ms/step - loss: 0.6394 - accuracy: 0.8664\n",
            "Epoch 149/200\n",
            "301/301 [==============================] - 76s 251ms/step - loss: 0.6085 - accuracy: 0.8699\n",
            "Epoch 150/200\n",
            "301/301 [==============================] - 78s 261ms/step - loss: 0.5836 - accuracy: 0.8759\n",
            "Epoch 151/200\n",
            "301/301 [==============================] - 74s 247ms/step - loss: 0.5543 - accuracy: 0.8832\n",
            "Epoch 152/200\n",
            "301/301 [==============================] - 75s 250ms/step - loss: 0.5375 - accuracy: 0.8842\n",
            "Epoch 153/200\n",
            "301/301 [==============================] - 76s 252ms/step - loss: 0.5099 - accuracy: 0.8931\n",
            "Epoch 154/200\n",
            "301/301 [==============================] - 74s 247ms/step - loss: 0.4842 - accuracy: 0.8985\n",
            "Epoch 155/200\n",
            "301/301 [==============================] - 74s 246ms/step - loss: 0.4693 - accuracy: 0.9027\n",
            "Epoch 156/200\n",
            "301/301 [==============================] - 73s 243ms/step - loss: 0.4443 - accuracy: 0.9022\n",
            "Epoch 157/200\n",
            "301/301 [==============================] - 75s 251ms/step - loss: 0.4301 - accuracy: 0.9089\n",
            "Epoch 158/200\n",
            "301/301 [==============================] - 73s 242ms/step - loss: 0.4102 - accuracy: 0.9127\n",
            "Epoch 159/200\n",
            "301/301 [==============================] - 73s 243ms/step - loss: 0.3949 - accuracy: 0.9144\n",
            "Epoch 160/200\n",
            "301/301 [==============================] - 73s 243ms/step - loss: 0.3813 - accuracy: 0.9180\n",
            "Epoch 161/200\n",
            "301/301 [==============================] - 75s 249ms/step - loss: 0.3693 - accuracy: 0.9194\n",
            "Epoch 162/200\n",
            "301/301 [==============================] - 73s 243ms/step - loss: 0.3619 - accuracy: 0.9221\n",
            "Epoch 163/200\n",
            "301/301 [==============================] - 73s 241ms/step - loss: 0.3419 - accuracy: 0.9259\n",
            "Epoch 164/200\n",
            "301/301 [==============================] - 74s 245ms/step - loss: 0.3253 - accuracy: 0.9277\n",
            "Epoch 165/200\n",
            "301/301 [==============================] - 75s 248ms/step - loss: 0.3185 - accuracy: 0.9301\n",
            "Epoch 166/200\n",
            "301/301 [==============================] - 76s 254ms/step - loss: 0.3076 - accuracy: 0.9318\n",
            "Epoch 167/200\n",
            "301/301 [==============================] - 76s 252ms/step - loss: 0.3059 - accuracy: 0.9333\n",
            "Epoch 168/200\n",
            "301/301 [==============================] - 79s 261ms/step - loss: 0.2907 - accuracy: 0.9364\n",
            "Epoch 169/200\n",
            "301/301 [==============================] - 76s 253ms/step - loss: 0.2707 - accuracy: 0.9388\n",
            "Epoch 170/200\n",
            "301/301 [==============================] - 77s 254ms/step - loss: 0.2690 - accuracy: 0.9382\n",
            "Epoch 171/200\n",
            "301/301 [==============================] - 76s 254ms/step - loss: 0.2632 - accuracy: 0.9394\n",
            "Epoch 172/200\n",
            "301/301 [==============================] - 72s 240ms/step - loss: 0.2598 - accuracy: 0.9396\n",
            "Epoch 173/200\n",
            "301/301 [==============================] - 71s 236ms/step - loss: 0.2496 - accuracy: 0.9400\n",
            "Epoch 174/200\n",
            "301/301 [==============================] - 72s 241ms/step - loss: 0.2430 - accuracy: 0.9430\n",
            "Epoch 175/200\n",
            "301/301 [==============================] - 72s 240ms/step - loss: 0.2350 - accuracy: 0.9436\n",
            "Epoch 176/200\n",
            "301/301 [==============================] - 73s 243ms/step - loss: 0.2332 - accuracy: 0.9435\n",
            "Epoch 177/200\n",
            "301/301 [==============================] - 72s 240ms/step - loss: 0.2280 - accuracy: 0.9441\n",
            "Epoch 178/200\n",
            "301/301 [==============================] - 77s 255ms/step - loss: 0.2213 - accuracy: 0.9449\n",
            "Epoch 179/200\n",
            "301/301 [==============================] - 75s 248ms/step - loss: 0.2147 - accuracy: 0.9456\n",
            "Epoch 180/200\n",
            "301/301 [==============================] - 72s 240ms/step - loss: 0.2101 - accuracy: 0.9469\n",
            "Epoch 181/200\n",
            "301/301 [==============================] - 71s 237ms/step - loss: 0.2093 - accuracy: 0.9466\n",
            "Epoch 182/200\n",
            "301/301 [==============================] - 74s 246ms/step - loss: 0.2104 - accuracy: 0.9456\n",
            "Epoch 183/200\n",
            "301/301 [==============================] - 74s 244ms/step - loss: 0.2007 - accuracy: 0.9473\n",
            "Epoch 184/200\n",
            "301/301 [==============================] - 72s 240ms/step - loss: 0.1959 - accuracy: 0.9501\n",
            "Epoch 185/200\n",
            "301/301 [==============================] - 71s 236ms/step - loss: 0.1946 - accuracy: 0.9486\n",
            "Epoch 186/200\n",
            "301/301 [==============================] - 74s 246ms/step - loss: 0.1938 - accuracy: 0.9475\n",
            "Epoch 187/200\n",
            "301/301 [==============================] - 74s 247ms/step - loss: 0.1882 - accuracy: 0.9490\n",
            "Epoch 188/200\n",
            "301/301 [==============================] - 74s 246ms/step - loss: 0.1889 - accuracy: 0.9484\n",
            "Epoch 189/200\n",
            "301/301 [==============================] - 73s 241ms/step - loss: 0.1818 - accuracy: 0.9502\n",
            "Epoch 190/200\n",
            "301/301 [==============================] - 74s 246ms/step - loss: 0.1824 - accuracy: 0.9498\n",
            "Epoch 191/200\n",
            "301/301 [==============================] - 74s 246ms/step - loss: 0.1811 - accuracy: 0.9486\n",
            "Epoch 192/200\n",
            "301/301 [==============================] - 73s 243ms/step - loss: 0.1784 - accuracy: 0.9493\n",
            "Epoch 193/200\n",
            "301/301 [==============================] - 74s 246ms/step - loss: 0.1819 - accuracy: 0.9489\n",
            "Epoch 194/200\n",
            "301/301 [==============================] - 73s 242ms/step - loss: 0.1765 - accuracy: 0.9482\n",
            "Epoch 195/200\n",
            "301/301 [==============================] - 74s 245ms/step - loss: 0.1762 - accuracy: 0.9482\n",
            "Epoch 196/200\n",
            "301/301 [==============================] - 73s 241ms/step - loss: 0.1695 - accuracy: 0.9496\n",
            "Epoch 197/200\n",
            "301/301 [==============================] - 71s 236ms/step - loss: 0.1669 - accuracy: 0.9498\n",
            "Epoch 198/200\n",
            "301/301 [==============================] - 76s 251ms/step - loss: 0.1707 - accuracy: 0.9504\n",
            "Epoch 199/200\n",
            "301/301 [==============================] - 72s 241ms/step - loss: 0.1661 - accuracy: 0.9508\n",
            "Epoch 200/200\n",
            "301/301 [==============================] - 75s 249ms/step - loss: 0.1602 - accuracy: 0.9499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test data\n",
        "test_data = df['titles'][0:500]\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
        "test_input = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "test_output = model.predict(test_input)\n",
        "\n",
        "# Evaluate the performance of the model on the test data\n",
        "test_loss, test_acc = model.evaluate(test_input, test_output)\n",
        "print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YITpBvv5aoCQ",
        "outputId": "319a53af-b4bd-4aef-dced-75337151230e"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 2s 88ms/step\n",
            "16/16 [==============================] - 2s 56ms/step - loss: 0.5311 - accuracy: 1.0000\n",
            "Test loss: 0.5311, Test accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "model.save('lstm_model_4.h5')\n",
        "files.download(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "wXcqOVzS9fEu",
        "outputId": "29175f36-1e85-4f88-fd45-407b3507f666"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7d0e6c50-6141-40d8-bd7c-e13d15926873\", \"lstm_model_4.h5\", 20946336)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tensorflow.keras.models import model_from_json\n",
        "model_json = model.to_json()\n",
        "with open(\"lstm_model_4.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# Download JSON file\n",
        "files.download(\"lstm_model_4.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "OxlBcMJyezUs",
        "outputId": "2dc34d5c-b5e4-4026-bc11-b1e5092d7de2"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_565e8be4-74ff-4ae5-837a-0f9aeb28bacd\", \"lstm_model_4.json\", 2957)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, input_text, max_length=20):\n",
        "    # initialize the generated output text with the input text\n",
        "    generated_text = input_text\n",
        "    # set the stop condition to False\n",
        "    stop_condition = False\n",
        "    while not stop_condition:\n",
        "        # tokenize the input text\n",
        "        input_sequence = tokenizer.texts_to_sequences([generated_text])[0]\n",
        "        # pad the input sequence\n",
        "        input_sequence = pad_sequences([input_sequence], maxlen=max_length-1, padding='pre')\n",
        "        # make a prediction\n",
        "        prediction = model.predict(input_sequence)[0]\n",
        "        # get the index of the predicted word\n",
        "        predicted_index = np.argmax(prediction)\n",
        "        # get the predicted word\n",
        "        predicted_word = tokenizer.index_word.get(predicted_index, '')\n",
        "        # check if we've generated the maximum length or found the end token\n",
        "        if len(generated_text.split()) == max_length or predicted_word == '':\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            # append the predicted word to the generated text\n",
        "            generated_text += ' ' + predicted_word\n",
        "    return generated_text.strip()\n"
      ],
      "metadata": {
        "id": "aq7q4_SPXSJp"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = 'rethinking the skip connections'\n",
        "generated_text = generate_text(model, tokenizer, input_text)\n",
        "print(generated_text)\n",
        "\n",
        "# visualize the predictions made by the model\n",
        "input_sequence = tokenizer.texts_to_sequences([input_text])[0]\n",
        "input_sequence = pad_sequences([input_sequence])\n",
        "prediction = model.predict(input_sequence)[0]\n",
        "predicted_indices = np.argsort(prediction)[::-1][:5]\n",
        "predicted_words = [tokenizer.index_word[i] for i in predicted_indices]\n",
        "predicted_probabilities = prediction[predicted_indices]\n",
        "for word, probability in zip(predicted_words, predicted_probabilities):\n",
        "    print(f'{word}: {probability:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwMuM1TIabCJ",
        "outputId": "5f7f8ccc-f09f-45c0-eace-29914d405afb"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "rethinking the skip connections in unet from a channelwise perspective with transformer of active contours for image segmentation based on\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "in: 0.98\n",
            "with: 0.01\n",
            "towards: 0.00\n",
            "for: 0.00\n",
            "and: 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with open('lstm_tokenizer.json', 'w', encoding='utf-8') as f:\n",
        "    f.write(tokenizer_json)\n",
        "\n",
        "files.download('lstm_tokenizer.json')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "mTtl5hMhfpAD",
        "outputId": "f7415da0-462d-4911-b878-69365188add5"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a74b5986-eff6-41c0-8fe7-418b8ddcdd0c\", \"lstm_tokenizer.json\", 208772)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Jc06zrEfSnv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}